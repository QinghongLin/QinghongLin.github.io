<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="shortcut icon" href="./figures/world.png">
  <link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
  <meta name="description" content="When Vision Meets Code ‚Äî Kevin Qinghong Lin">
  <title>When Vision Meets Code ‚Äî Kevin Qinghong Lin</title>
  <style>
    body {
      max-width: 860px;
      margin: 0 auto;
      padding: 0 20px;
    }

    #layout-content {
      margin-top: 30px;
    }

    .back-link {
      font-size: 0.92em;
      margin-bottom: 18px;
      display: block;
    }

    .blog-meta {
      color: #888;
      font-size: 0.92em;
      margin-bottom: 28px;
      border-bottom: 1px solid #eee;
      padding-bottom: 12px;
    }

    blockquote {
      border-left: 3px solid #6495ED;
      margin: 0 0 18px 0;
      padding: 6px 16px;
      color: #555;
      font-style: italic;
      background: #f9f9f9;
    }

    pre {
      background: #f5f5f5;
      border: 1px solid #ddd;
      border-radius: 4px;
      padding: 12px;
      font-size: 0.82em;
      overflow-x: auto;
      line-height: 1.5;
      white-space: pre-wrap;
      word-break: break-all;
    }

    code {
      background: #f5f5f5;
      padding: 1px 5px;
      border-radius: 3px;
      font-size: 0.9em;
    }

    pre code {
      background: none;
      padding: 0;
    }

    .section-divider {
      border: none;
      border-top: 1px solid #eee;
      margin: 28px 0;
    }

    /* ‚îÄ‚îÄ Table of Contents ‚îÄ‚îÄ */
    .toc {
      background: #f7f9ff;
      border: 1px solid #d8e4f8;
      border-radius: 6px;
      padding: 16px 22px;
      margin: 24px 0 32px;
      font-size: 0.92em;
      display: inline-block;
      min-width: 260px;
    }

    .toc-title {
      font-weight: bold;
      font-size: 0.96em;
      margin-bottom: 10px;
      color: #333;
      letter-spacing: 0.02em;
    }

    .toc ol {
      margin: 0;
      padding-left: 20px;
    }

    .toc li {
      margin: 5px 0;
      line-height: 1.5;
    }

    .toc a {
      color: #4a72c4;
      text-decoration: none;
    }

    .toc a:hover {
      text-decoration: underline;
    }

    /* sub-items (h3) */
    .toc ol ol {
      padding-left: 18px;
      list-style-type: lower-alpha;
      margin-top: 4px;
    }

    .toc ol ol li {
      margin: 3px 0;
      font-size: 0.95em;
      color: #555;
    }

    /* Media tables */
    .media-table {
      width: 100%;
      border-collapse: collapse;
      text-align: center;
      margin: 20px auto;
    }

    .media-table th {
      text-align: center;
      padding: 8px 12px;
      font-weight: bold;
      background: #f7f7f7;
      border-bottom: 2px solid #ddd;
    }

    .media-table td {
      text-align: center;
      padding: 10px 8px;
      vertical-align: top;
    }

    .media-wrap {
      width: 100%;
      max-width: 280px;
      aspect-ratio: 16 / 9;
      overflow: hidden;
      margin: 0 auto;
      border: 1px solid #ddd;
      border-radius: 4px;
    }

    .media-wrap img,
    .media-wrap video {
      width: 100%;
      height: 100%;
      object-fit: cover;
      display: block;
    }

    .media-caption {
      font-size: 0.82em;
      color: #666;
      margin-top: 6px;
      font-style: italic;
    }

    /* SVG / code comparison table */
    .example-table {
      width: 100%;
      border-collapse: collapse;
      margin: 18px 0;
      font-size: 0.88em;
    }

    .example-table th {
      background: #f7f7f7;
      border: 1px solid #ddd;
      padding: 8px 12px;
      text-align: center;
    }

    .example-table td {
      border: 1px solid #ddd;
      padding: 10px 12px;
      vertical-align: top;
    }

    .example-table td pre {
      margin: 0;
      font-size: 0.78em;
    }

    .example-table td img {
      max-width: 160px;
      display: block;
      margin: 0 auto;
    }

    .example-table td em {
      display: block;
      text-align: center;
      color: #888;
      font-size: 0.85em;
      margin-top: 5px;
    }

    h2 {
      margin-top: 36px;
    }

    h3 {
      margin-top: 24px;
    }

    p {
      text-align: justify;
      line-height: 1.65;
    }

    .references {
      font-size: 0.88em;
    }

    .references li {
      margin-bottom: 4px;
    }

    .center-img {
      display: block;
      margin: 18px auto;
      max-width: 100%;
    }

    .insight-box {
      background: #f0f5ff;
      border-left: 3px solid #6495ED;
      padding: 12px 16px;
      margin: 18px 0;
      border-radius: 0 4px 4px 0;
    }

    /* Results table */
    .results-table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      font-size: 0.88em;
    }

    .results-table th {
      background: #f0f5ff;
      border: 1px solid #c8d8f0;
      padding: 8px 12px;
      text-align: center;
      font-weight: bold;
    }

    .results-table td {
      border: 1px solid #ddd;
      padding: 7px 12px;
      text-align: center;
    }

    .results-table tr:hover {
      background: #fafcff;
    }

    .results-table .best {
      font-weight: bold;
      color: #2a5ab8;
    }

    .results-table .upper-bound {
      font-style: italic;
      color: #888;
      background: #f9f9f9;
    }

    .results-table .highlight {
      background: #eef4ff;
    }

    /* Draw-this note */
    .draw-note {
      background: #fffbe6;
      border: 1px dashed #e6c619;
      border-radius: 6px;
      padding: 12px 16px;
      margin: 18px 0;
      font-size: 0.87em;
      color: #7a6000;
    }

    .draw-note strong {
      color: #5a4400;
    }
  </style>
</head>

<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<body>
  <div id="layout-content">

    <a class="back-link" href="https://qhlin.me/">Kevin‚Äôs Homepage</a>

    <div id="toptitle">
      <h1>When Vision Meets Code</h1>
    </div>

    <div class="blog-meta">
      writing: Kevin Qinghong Lin, Claude Sonnet 4.5</br>
      image: Gemini-3-Pro</br>
      website: Claude Sonnet 4.5</br> 
    </div>

    <p>
Unlike <code>Language</code>, which is free-form, flexible, and descriptive, <code>Code</code> is precise, structured, and executable. It doesn't merely describe what should happen; it makes it happen. And while code has long been treated as a specialized modality, its role has become central to programming and tool use, especially in the age of agents. <code>Vision</code>, by contrast, is how humans perceive the physical world directly and intuitively ‚Äî raw, unstructured, and immediate.
    </p>

    <img class="center-img" src="./assets/teaser.png" alt="teaser" style="max-width: 740px;">

    <p>
      Something interesting is happening at the frontier. The <a href="https://x.com/lepadphone/status/1988562131390988796">Gemini-3</a>
      uses SVG animation to highlight its strong multimodal capability.
      <a href="https://www.kimi.com/blog/kimi-k2-5.html">Kimi K2.5</a> emphasizes visual agentic intelligence such as
      reconstructing a website from visual demonstrations. Together, these point to a growing intuition:
      <code>code and vision are no longer isolated and they are finding each other.</code>
    </p>
<!-- 
    <table class="media-table" style="margin: 40px auto; width: 100%; text-align: center;">
      <tr>
        <th style="width: 50%;">Gemini OS</th>
        <th style="width: 50%;">Kimi K2.5</th>
      </tr>
      <tr>
        <td style="width: 50%; padding: 10px;">
          <video controls style="width: 100%; max-width: 100%;">
            <source src="assets/product/gemini.mp4" type="video/mp4">
          </video>
        </td>
        <td style="width: 50%; padding: 10px;">
          <video controls style="width: 100%; max-width: 100%;">
            <source src="assets/product/kimi.mp4" type="video/mp4">
          </video>
        </td>
      </tr>
    </table> -->

  <p>
    As a computer vision and agent researcher, I have become deeply fascinated by this intersection. 
    I want to share some of my thoughts and recent works on <code>what happens when vision meets code</code>: 
    how code can offer a new lens for images, video, 3D, and world models.
  </p>

    <!-- ==================== TABLE OF CONTENTS ==================== -->
    <nav class="toc">
      <div class="toc-title">Contents</div>
      <ol>
        <li><a href="#code-as-visual-representation">Code as Visual Representation</a></li>
        <li>
          <a href="#video-generation-via-programming">Video Generation via Programming</a>
<!--           <ol>
            <li><a href="#real-world-applications">Real-world Applications</a></li>
          </ol> -->
        </li>
        <li>
          <a href="#coder-as-world-model">Coder as World Model</a>
          <ol>
            <li><a href="#3d-blender-code-as-engine">3D: Blender code engine</a></li>
            <li><a href="#gui-html-code-as-engine">GUI: HTML code engine</a></li>
          </ol>
        </li>
        <li><a href="#looking-forward">Looking Forward</a></li>
        <li><a href="#references">References</a></li>
      </ol>
    </nav>

    <!-- ==================== SECTION 1 ==================== -->
    <h2 id="code-as-visual-representation">Code as Visual Representation</h2>

    <p>
      AI models typically perceive images the way cameras capture them: as a grid of millions of pixels, 
      each storing an exact RGB value. It works but it's not really how humans see. When we glance at 
      a scene, we don't mentally log every pixel. We notice structure: where things are, how many there 
      are, what shapes they form.
    </p>
    <p>
      Think of how an artist starts with a rough sketch before adding color and detail. That sketch is 
      sparse, symbolic, structural; is often enough to convey meaning. This gave us an idea: what if, 
      instead of pixels, we <code>represent images via code representation</code>?
    </p>
    <p>
      Scalable Vector Graphics (SVG) being a choice, it is the format behind most crisp icon, logo on the web. Rather than storing colors 
      pixel-by-pixel, it describes shapes: <em>draw a blue rectangle here, a circle there, a curved path 
      connecting them</em>. It's compact, human-readable, and notably executable. 
      <!-- Which raises a  curious question: <strong>can an AI learn to see and reason through code, rather than pixels?</strong> -->
    </p>
    <p>
      Here's a quick example with Peppa Pig. The image below is rendered entirely from SVG code, yet 
      we recognize her easily.
    </p>

    <table class="example-table">
      <thead>
        <tr>
          <th>Input Image</th>
          <th>Human‚Äôs sketch</th>
          <th>SVG Code</th>
          <th>Rendered Image</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align:center; width:22%;">
            <img src="assets/vcode/image.jpg" alt="Input image">
          </td>
          <td style="text-align:center; width:22%;">
            <img src="assets/vcode/sketch.png" alt="Human‚Äôs sketch">
          </td>
          <td style="width:22%;">
<pre>&lt;svg viewBox="0 0 320 190" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;rect x="0" y="0" width="320" height="190" fill="#7ec0ee"/&gt;
  &lt;rect x="0" y="120" width="320" height="70" fill="#72c45a"/&gt;
  ...
&lt;/svg&gt;</pre>
          </td>
          <td style="text-align:center; width:22%;">
            <img src="assets/vcode/render.svg" alt="Rendered SVG">
          </td>
        </tr>
      </tbody>
    </table>

    <p>
    This is the core idea behind <a href="https://csu-jpg.github.io/VCode/">VCode</a>: if a model truly understands an image, it should be able to 
reconstruct it in SVG; not a pixel-perfect copy, but a symbolic one. The objects, their positions, 
their relationships, their hierarchy, all expressed as clean, human-readable vector primitives.
    </p>

<figure style="text-align: center;">
  <img class="center-img" src="./assets/vcode/teaser.png" alt="vcode teaser" style="max-width: 800px;">
  <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">VCode is inspired by the <strong>autoencoder</strong> framework except (i) the intermediate representation is code rather than a latent embedding, and (ii) reconstruction targets symbolic fidelity rather than pixel-wise MSE.</figcaption>
</figure>

<p>
  <strong>How to evaluate</strong> whether the code actually captured what matters, symbolically? 
  This motivate us to devise a new evaluation prototype named CodeVQA: convert an image to SVG, render it back, then ask a policy model to answer questions using <em>only</em> the rendered version. 
  If the answers match those from the original image, the SVG preserved meaning not just appearance.
</p>
<figure style="text-align: center;">
  <img class="center-img" src="./assets/vcode/codevqa.png" alt="codevqa teaser" style="max-width: 540px;">
  <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">
    CodeVQA evaluates whether a rendered SVG retains enough semantic content to answer visual 
    questions as accurately as the original image.
  </figcaption>
</figure>

<p>
  Here is a more telling example. Among the models, Gemini-3-Pro best preserves the key detail ‚Äî 
  <u>the rhino horn</u> which is precisely what makes the image funny.</br>
  Using CodeVQA, we can measure how well foundation models understand and reason 
  about visual-naive coding ability. If you are curious, check out the <a href="https://csu-jpg.github.io/VCode/">website</a> for more details.
</p>

<figure>
  <table class="example-table">
    <caption style="color: #666; font-size: 0.9em; margin-bottom: 8px;"><strong>Q</strong>: What is funny about this image?</caption>
    <thead>
      <tr>
        <th>orig. image</th>
        <th>by Gemini-3-proüèÖ</th>
        <th>by Claude-opus-4</th>
        <th>by GPT-5</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align:center; width:22%;">
          <img src="assets/vcode/Original_v1_153.jpg" alt="Input image">
        </td>
        <td style="text-align:center; width:22%;">
          <img src="assets/vcode/Gemini-3-pro_v1_153.svg" alt="Input image">
        </td>
        <td style="text-align:center; width:22%;">
          <img src="assets/vcode/Claude-Opus-4_v1_153.svg" alt="Input image">
        </td>
        <td style="text-align:center; width:22%;">
          <img src="assets/vcode/GPT-5_v1_153.svg" alt="Input image">
        </td>
      </tr>
    </tbody>
  </table>
  <figcaption style="text-align: center; color: #666; font-size: 0.9em; margin-top: 8px;"><strong>A</strong>: It is a cartoon of a rhinoceros painting a picture and each picture has <u>its rhino horn because the rhino horn grows in front of its eyes</u>. The caption ‚ÄúYou see the world as you are!‚Äù is a playful commentary on how our perspective shapes our perception of the world.</figcaption>
</figure>

    <!-- ==================== SECTION 2 ==================== -->
    <h2 id="video-generation-via-programming">Video Generation via Programming</h2>

<p>
If code can represent a static image, can it represent a dynamic video? Intutively, time is something code handles naturally through sequences, loops, and state transitions.
</p>
<p>
Video generation models are remarkably good at the natural world, simulating physical laws 
like how light scatters, fabric folds, or smoke drifts. But there is another world entirely: 
explainer videos, data visualizations, motion graphics, professional animations. 
</p>
<p>
Take 
<strong>educational videos</strong> as a representative example. The building blocks are 
text, equations, shapes, and transitions, where ‚Äúapproximately correct‚Äù is unacceptable. 
As shown on the left, Veo3 struggles with <u>text fidelity and smooth transitions</u>. In contrast, 
code-based animation gives full control over every element (characters, lines, points) 
making precision not just possible, but guaranteed.
</p>

<table class="media-table">
  <caption style="color: #666; font-size: 0.9em; margin-bottom: 8px;">
    Comparison of pixel generation (Veo3) vs. code animation (Claude Opus 4.5) on instruction: ‚ÄúExplain what is LLM‚Äù
  </caption>
  <thead>
    <tr>
      <th>Pixel Generation</th>
      <th>Code Animation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
<div class="media-wrap" style="min-width: 360px;">
          <img src="assets/code2video/veo3.gif" alt="Veo3 generation">
        </div>
      </td>
      <td>
<div class="media-wrap" style="min-width: 360px;">
          <img src="assets/code2video/claude.gif" alt="Claude generation">
        </div>
      </td>
    </tr>
  </tbody>
</table>

<p>
  This is the motivation behind <a href="https://showlab.github.io/Code2Video/">Code2Video</a>. 
  Focusing on educational video, we reframe video creation as code animation ‚Äî 
  writing executable Python scripts using the <a href="https://www.manim.community/">Manim</a> 
  animation library. Every element is precisely specified: formula layout, text, animation and 
  transition ordering. Every real content lives in the code.
</p>

<p>
<strong>How to debug based on visual outcome?</strong>
When code breaks in an editor like VSCode, you get a clear signal (a stack trace, a line number, telling you exactly what went wrong). But debugging a visual animation is much harder for two reasons: (i) errors don‚Äôt crash the program, they just look improper, a misplaced coordinate or a timing mistake renders silently with no feedback; and (ii) it‚Äôs difficult to even describe what‚Äôs wrong and how to adjust such as should the object move left or right? How far? How do you put that into words?
</p>

<figure style="display: flex; justify-content: center; gap: 24px; align-items: flex-start;">
  <figure style="text-align: center; margin: 0;">
    <img class="center-img" src="./assets/code2video/issue.jpg" alt="vcode teaser left" style="max-width: 380px;">
    <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;"><strong>Challenges:</strong> how to trace the visual effect from code?</figcaption>
  </figure>
  <figure style="text-align: center; margin: 0;">
    <img class="center-img" src="./assets/code2video/solution.png" alt="vcode teaser right" style="max-width: 360px;">
    <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;"><strong>Solution:</strong> Visual prompts make debugging more effectively (‚Äúmove the cat from D2 to B2.‚Äù).</figcaption>
  </figure>
</figure>

<p>
To solve this, we introduce <u>anchor points as visual prompts</u>: labeled markers overlaid directly on the animation, with each marker's position tracked in the code. When something looks off, the Critic agent can reference these markers to describe the problem clearly. For example, ‚Äúmove the cat from D2 to B2‚Äù then jump straight to the relevant line in the code to fix it.
</p>

<p>
  <strong>What make a good educational video?</strong> Evaluating educational video quality is not just about whether the video looks good it's about whether it actually <u>teaches</u>. We introduce <strong>TeachQuiz</strong>, where a student model watches a generated video and answers a quiz about the target concept. However, simply measuring quiz accuracy is not enough: top-performing models can often answer correctly without watching the video at all, relying on knowledge they already have. To isolate what the video truly taught, we first apply an <u>unlearning</u> step to let the model forget this part knowledge, then re-expose it to the video. The final score measures the improvement, capturing only the knowledge gained from the video itself, not from base model.
</p>

<figure style="text-align: center;">
  <img class="center-img" src="./assets/code2video/eval.png" alt="code2video eval" style="max-width: 760px;">
  <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Good educational videos foucs on teaching knowledge.</figcaption>
</figure>

<p>
üë¶ Fun fact: We conducted a human study by showing the generated videos to middle school students and received positive feedback. We believe AI can play a promising important role in education.
</p>

    <h3 id="real-world-applications">Real-world Impacts</h3>

 <p>
  Lastly, I would like to express our sincere gratitude to
  <a href="https://www.3blue1brown.com/">3Blue1Brown</a>, whose work inspired the core idea behind this project and has made tremendous contributions to the field of educational animation.
  We also draw inspiration from <a href="https://github.com/remotion-dev/remotion">Remotion</a>, an open-source framework for building programmatic video with React.
  Here's a glimpse of what they produce:
</p>

<table class="media-table" style="text-align: center;">
  <thead>
    <tr>
      <th>3Blue1Brown</th>
      <th>Remotion</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <div class="media-wrap" style="max-width:480px; margin: 0 auto;">
          <img src="assets/product/3b1b.gif" alt="3Blue1Brown example">
        </div>
        <p class="media-caption" style="text-align: center;">‚ÄúHow might LLMs store facts‚Äù, made by 3B1B team.</p>
      </td>
      <td>
        <div class="media-wrap" style="max-width:480px; margin: 0 auto;">
          <img src="assets/product/remotion.gif" alt="Remotion example">
        </div>
        <p class="media-caption" style="text-align: center;">Instruction: ‚ÄúThis video was made with code‚Äù</p>
      </td>
    </tr>
  </tbody>
</table>


    <!-- ==================== SECTION 3 ==================== -->
    <h2 id="coder-as-world-model">Coder as World Model</h2>

<p>
  Above we explored how code can represent visual content. But code can do something even more powerful: it can <strong>simulate environments</strong>. While world models are commonly known through video generation, we want to offer a different perspective that code itself acts as the engine.
</p>

<h3 id="3d-blender-code-as-engine">3D: Blender Code as Engine</h3>
<p>
  3D worlds are far more complex than 2D where objects have physics, depth, collision, and interaction. <a href="https://docs.blender.org/api/current/index.html">Blender</a> provides a powerful programmable engine to model all of this.</br>
  Built on top of it, <a href="https://fugtemypt123.github.io/VIGA-website/">VIGA</a> is a multimodal agent that reconstructs any input image as an editable 3D scene, where every element, from object placement to lighting to physical interactions, is controllable through blender code. Moreover, VIGA can simulate what happens next: throw a ball to knock over objects, simulate an earthquake, or break a mirror, all driven by generated Blender code.
</p>

<table class="media-table">
  <thead>
<caption style="caption-side: bottom; text-align: center; color: #666; font-size: 0.9em; padding-top: 8px;">
  VIGA reconstructs a 3D scene from a single image and simulates physical interactions, all driven by generated Blender code.
</caption>
    <tr>
      <th>Input</th>
      <th>Reconstruct the scene</th>
      <th>Throw a ball to knock over all the objects</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>
        <div class="media-wrap" style="max-width: 320px; margin: 0 auto;">
          <img src="assets/viga/target.png" style="width: 100%;">
        </div>
      </td>
      <td>
        <div class="media-wrap" style="max-width: 320px; margin: 0 auto;">
          <video autoplay loop muted playsinline style="width: 100%;">
            <source src="assets/viga/render.mp4" type="video/mp4">
          </video>
        </div>
      </td>
      <td>
        <div class="media-wrap" style="max-width: 320px; margin: 0 auto;">
          <video autoplay loop muted playsinline style="width: 100%;">
            <source src="assets/viga/art.mp4" type="video/mp4">
          </video>
        </div>
      </td>
    </tr>
  </tbody>
</table>

    <h3 id="gui-html-code-as-engine">GUI: HTML Code as Engine</h3>

<p>
  Another dimension is the digital world. Autonomous GUI agents that navigate apps, click buttons, and fill forms need to predict what happens before they act just as humans mentally simulate the outcome of an action before committing to it. </br>
This predictive capability is a GUI world model: given the current screen and a proposed action, simulate what the next screen will look like. One natural solution is to predict the next state in text<sup><a href="https://arxiv.org/pdf/2411.06559">[1]</a></sup> or visual<sup><a href="https://neural-os.com/">[2]</a><a href="https://ai-agents-2030.github.io/ViMo/">[3]</a></sup> but text loses rich visual information, while visual diffusion models struggle with rendering accurate text. <a href="https://neural-os.com/">NeuralOS</a> and <a href="https://ai-agents-2030.github.io/ViMo/">ViMo</a> are interesting works that pioneers this direction.
</p>


<div style="display: flex; justify-content: center; gap: 24px; align-items: flex-start; margin: 24px auto;">
  <figure style="text-align: center; max-width: 360px; margin: 0;">
    <video autoplay loop muted playsinline style="width: 100%; border-radius: 8px;">
      <source src="assets/code2world/neuralos.mp4" type="video/mp4">
    </video>
    <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">
      NeuralOS trains a diffusion model to simulate the next screenshot.
    </figcaption>
  </figure>

  <figure style="text-align: center; max-width: 360px; margin: 0;">
    <img src="assets/code2world/errorcase.png" style="width: 100%; border-radius: 8px;">
    <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">
    Diffusion model faces challenges in accurately rendering text.
    </figcaption>
  </figure>
</div>

<p>
  A strong motivation is that GUIs (such as website) are usually defined by code engine (such as HTML).
  <strong>Next-state prediction as renderable code generation</strong> is thus a natural fit, <a herf="https://amap-ml.github.io/Code2World/">Code2World</a> trains a vision-language coder to synthesize the next screen as HTML, conditioned on the current screenshot and the proposed action, then renders it directly in a browser.
</p>

<figure style="text-align: center;">
  <img class="center-img" src="./assets/code2world/teaser.png" alt="code2world teaser" style="max-width: 512px;">
  <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">Given a current GUI observation and an action, Code2World predicts the next screenshot via renderable code generation.</figcaption>
</figure>

<p>
  <strong>How to use a Code World Model?</strong> A good world model should ultimately empower the agent that uses it. Rather than evaluating visual quality alone, we focus on whether the predicted next state is accurate enough to guide action selection tested in GUI environments. By acting as a critic that previews the outcome of each candidate action, the world model helps the agent pick the right one, improving task success rate.
</p>

<div style="display: flex; justify-content: center; gap: 24px; align-items: flex-start; margin: 24px auto;">
  <figure style="text-align: center; max-width: 500px; margin: 0;">
    <img src="assets/code2world/vis.png" style="width: 100%; border-radius: 8px;">
    <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">
    Next-state prediction by different coder baseline.
    </figcaption>
  </figure>

  <figure style="text-align: center; max-width: 400px; margin: 0;">
    <img src="assets/code2world/exps.png" style="width: 100%; border-radius: 8px;">
    <figcaption style="margin-top: 8px; color: #666; font-size: 0.9em;">
    Code2World increase agent‚Äôs success rate in online AndroidWorld.
    </figcaption>
  </figure>
</div>


  <h3 id="real-world-applications">Real-world Impacts</h3>

 <p>
 Google's <a href="https://aistudio.google.com/apps/bundled/gemini_os?showPreview=true&showAssistant=true">Gemini OS</a> is a prototype that simulates a generative operating system where every screen is generated in real time by programming. Each click triggers the model to build the next UI on the fly, creating an infinite, adaptive computer experience.
</p>

<!-- <div style="text-align: center;">
  <video width="560" controls>
  <source src="assets/code2world/geminios.mp4" type="video/mp4">
  </video>
</div>
 -->
 
<!-- ==================== LOOKING FORWARD ==================== -->
    <h2 id="looking-forward">Looking forward</h2>

<div class="insight-box">
  <strong>Coders should be natively multimodal.</strong><br>
  Future coders should receive both text instructions and visual (even video) demonstrations simultaneously so that writing code that reproduces visual effects by bridging language and vision through programs<sup><a href="https://arxiv.org/pdf/2403.03163">[1]</a></sup><sup><a href="https://arxiv.org/abs/2510.23538">[2]</a></sup>.
</div>

<div class="insight-box">
  <strong>How to measure visual animation outcome.</strong><br>
  Complex visual output is inherently difficult to evaluate. Defining a reliable outcome metric is critical ‚Äî especially for reinforcement learning<sup><a href="https://arxiv.org/pdf/2505.20793">[1]</a></sup>: how do we design a proper reward signal, such as a rubric-based reward, that guides the model toward producing correct visual effects?
</div>

<div class="insight-box">
  <strong>Visual understanding for Code.</strong><br>
  For human, reading code is inherently a visual task. How to enable model that can see code<sup><a href="https://www.arxiv.org/pdf/2602.01785">[1]</a></sup><sup><a href="https://www.arxiv.org/pdf/2602.00746">[2]</a></sup> not just parse it as text ‚Äî gains access to layout cues, indentation, and visual highlights that carry real semantic weight. 
</div>

<p>
I am grateful to all collaborators who contributed to the works. Thank the broader research and open-source community whose shared tools, datasets, and ideas.
</p>

    <!-- ==================== REFERENCES ==================== -->
    <hr class="section-divider">
    <h2 id="references">References</h2>

    <ul class="references">
      <li>‚ÄúVCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation.‚Äù [<a href="https://csu-jpg.github.io/VCode/">website</a>]</li>
      <li>‚ÄúCode2Video: A Code-centric Paradigm for Educational Video Generation.‚Äù [<a href="https://showlab.github.io/Code2Video/">website</a>]</li>
      <li>‚ÄúCode2World: A GUI World Model via Renderable Code Generation.‚Äù [<a href="https://amap-ml.github.io/Code2World/">website</a>]</li>
      <li>‚ÄúA Vision Check-up for Language Models.‚Äù [<a href="http://vision-checkup.github.io/">website</a>]</li>
      <li>‚ÄúStarVector: Generating Scalable Vector Graphics Code From Images And Text.‚Äù [<a href="https://starvector.github.io/starvector/">website</a>]</li>
      <li>‚ÄúRendering-Aware Reinforcement Learning for
Vector Graphics Generation.‚Äù [<a href="https://arxiv.org/pdf/2505.20793">paper</a>]</li>
      <li>‚ÄúSymbolic Graphics Programming with Large Language Models‚Äù [<a href="https://arxiv.org/pdf/2509.05208">website</a>]</li>
      <li>‚ÄúTheoremExplainAgent: Towards Video-based Multimodal Explanations for LLM Theorem Understanding‚Äù [<a href="https://tiger-ai-lab.github.io/TheoremExplainAgent/">website</a>]</li>
      <li>‚ÄúMeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds‚Äù [<a href="https://daibingquan.github.io/MeshCoder/">website</a>]</li>
      <li>‚ÄúVIGA: Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning.‚Äù [<a href="https://fugtemypt123.github.io/VIGA-website/">website</a>]</li>
      <li>‚ÄúIs Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents.‚Äù [<a href="https://arxiv.org/pdf/2411.06559">paper</a>]</li>
      <li>‚ÄúNeuralOS: Towards Simulating Operating Systems via Neural Generative Models‚Äù [<a href="https://neural-os.com/">website</a>]</li>
      <li>‚ÄúViMo: A Generative Visual GUI World Model for App Agent.‚Äù [<a href="https://ai-agents-2030.github.io/ViMo/">website</a>]</li>
      <li>‚ÄúDesign2Code: Benchmarking Multimodal Code Generation for Automated Front-End Engineering.‚Äù [<a href="https://arxiv.org/pdf/2403.03163">paper</a>]</li>
      <li>‚ÄúJanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence.‚Äù [<a href="https://arxiv.org/abs/2510.23538">paper</a>]</li>
      <li>‚ÄúCodeOCR: On the Effectiveness of Vision Language Models in Code Understanding.‚Äù [<a href="https://www.arxiv.org/pdf/2602.01785">paper</a>]</li>
      <li>‚ÄúLongCodeOCR.‚Äù [<a href="https://www.arxiv.org/pdf/2602.00746">paper</a>]</li>
    </ul>

    <br>
    <div align="center">
      <font color="gray">&copy; Kevin</font>
    </div>
    <br>

  </div>
</body>
</html>
