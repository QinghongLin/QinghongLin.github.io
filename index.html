<!DOCTYPE html>
<!-- saved from url=(0025)https://qinghonglin.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="shortcut icon" href="https://qinghonglin.github.io/myIcon.ico">
	<meta name="google-site-verification" content="PcjE-PoDvp7KoKeZ5wE1g_BU8VI5wioTfiAgbIst__4" />
	<meta name="keywords" content="Qinghong Lin">
	<meta name="description" content="Qinghong Lin&#39;s homepage">
	<!-- <link rel="icon" href="icon.ico" type="figures/emoji"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/technologist-light-skin-tone_1f9d1-1f3fb-200d-1f4bb.png"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/snowflake_2744-fe0f.png"> -->
	<link rel="shortcut icon" href="./figures/icon.png">
	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<title>Qinghong Lin @ National University of Singapore</title>
	<script async="" src="./index_files/analytics.js"></script>
	<script type="text/javascript" async="" src="./index_files/ga.js"></script>
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-39824124-1']);
		_gaq.push(['_trackPageview']);
		(function () {
			var ga = document.createElement('script');
			ga.type = 'text/javascript';
			ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0];
			s.parentNode.insertBefore(ga, s);
		})();
	</script>
</head>

<body>
	<div id="layout-content" style="margin-top:25px">
		<!-- <table> -->
		<table style="margin-bottom: -20px;">
			<tbody>
				<tr>
					<td width="670">
						<div id="toptitle">

							<!-- <h1>Kevin Qinghong Lin</h1> -->
							<h1><span style="font-family: 'Comic Sans MS', Georgia; color: #6495ED;">Kevin</span> Qinghong Lin</h1>
						</div>
						<h3>Ph.D. Student</h3>
						<p>
							<a href="https://sites.google.com/view/showlab">Show Lab</a><br />
							<a href="https://www.nus.edu.sg/">National University of Singapore</a><br />
							<br>
							Email: <u><a href="mailto:kevin.qh.lin@gmail.com">kevin.qh.lin [at] gmail.com</a></u>
						</p>
						<p>
							<a href="https://github.com/QinghongLin"><img src="./index_files/github.png"
									height="30px"></a>
							<a href="https://scholar.google.com/citations?user=EvbGjlUAAAAJ&hl=zh-CN"><img
									src="./index_files/google_scholar.png" height="30px"></a>
							<a href="https://www.linkedin.com/in/lqh/"><img src="./index_files/linkedin.png"
									height="30px"></a>
							<a href="https://twitter.com/KevinQHLin"><img src="./index_files/twitter.png"
									height="30px"></a>
						</p>
					</td>
					<td>
						<img src="./figures/kevin.jpeg" border="0" width="250"><br>
						<!-- <img src="./figures/kevin.jpeg" border="0" width="250"><br> -->
						<p style="font-size: 10px; font-family: 'Comic Sans MS', Georgia; text-align: left;">Photo taken on <a href="https://en.wikipedia.org/wiki/Rottnest_Island">Rottnest Island</a>.</p>
					</td>
				</tr>
				<tr>
				</tr>
			</tbody>
		</table>

		<h2>Biography</h2>
		<p>
		</p>
		<div style="text-align:justify">
			Hi, I am a Ph.D. student in <a href="https://sites.google.com/view/showlab">Show Lab @ NUS</a>, working with <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl">Prof. Mike
				Shou</a>.
			<p></p>
			<p>My research focuses on <a style="color:#224B8D;">Video Understanding</a> and <a style="color:#224B8D;">Language Models</a>, aiming to develop assistants to streamline human tasks.</p>
			<h2>News</h2>
			<ul>
				<li>
				    2024 Jun: Check out our new work on computer automation: <a href="https://showlab.github.io/videogui/">VideoGUI</a>.
				</li>
				<li>
				    2024 Jun: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> received <a href="https://egovis.github.io/awards/2022_2023/" style="color:#FA8072;">Egocentric Vision (EgoVis) Distinguished Paper Award</a>.
				</li>
				<li>
					2024 May: Recognized as <a href="https://x.com/CVPR/status/1793616950314369239">CVPR 2024 Outstanding Reviewers</a>.
				</li>
				<li>
					2024 Feb: <a href="https://openaccess.thecvf.com//content/CVPR2024/papers/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.pdf">VideoLLM-online</a>, <a href="https://arxiv.org/abs/2312.01987">SparseFormer</a> got accepted by <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
				</li>
				<li>
					2023 Sept: <a href="https://arxiv.org/abs/2305.13777">VisorGPT</a> got accepted by <a href="https://nips.cc/">NeurIPS 2023</a>.
				</li>
				<li>
					2023 Aug: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> received <a href="http://www.premiasg.org/", style="color:#FA8072;">PREMIA Best Student Paper Award (Gold award)</a>.
				</li>
				<li>
					2023 July: <a href="https://arxiv.org/abs/2307.16715">UniVTG</a>, <a href="https://arxiv.org/abs/2307.05463">EgoVLPv2</a>, <a href="https://arxiv.org/abs/2305.20087">TL;DR</a> got accepted by <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.
				</li>
				<li>
					2023 Mar: <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.html">All-in-one</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Affordance_Grounding_From_Demonstration_Video_To_Target_Image_CVPR_2023_paper.html">Afformer</a> got accepted by <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
				</li>
				<li>
					2022 Sept: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> got accepted by <a href="https://nips.cc/">NeurIPS 2022</a> as
					<a style="color:#FA8072">Spotlight</a>.
				</li>
				<li>
					2022 Aug: Joined <a href="https://sites.google.com/view/showlab">Show Lab @ NUS</a> to start my Ph.D. journey!
				</li>
					<li>
					2022 Jun: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> won Double Champions of 
					<a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">Joint 1st
						Ego4D and 10th EPIC Workshop, CVPR 2022</a>. <a
						href="https://cde.nus.edu.sg/ece/news/double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges/">[News]</a>
				</li>
			</ul>

			<h2>Preprints</h2>
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="260">
							<img src="figures/videogui.jpg" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://showlab.github.io/videogui/"><b>VideoGUI: A Benchmark for GUI Automation from Instructional Videos</b></a><br>
								<u>Kevin QH. Lin</u>, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2024<br>
								[<a href="https://showlab.github.io/videogui/">project</a>]
								[<a href="https://showlab.github.io/videogui/assets/preprint.pdf">paper</a>]
								[<a href="https://github.com/showlab/videogui">code</a>]								
								<br>
								<a style="color:#FA8072">Can an agent recreate PowerPoint animation effects from instructional videos?</a><br>

						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/cosmo.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2401.00849"><b>CosMo: Contrastive Streamlined Multimodal Model With Interleaved Pre-Training</b></a> <br>
							Alex JP. Wang, Linjie Li, <u>Kevin QH. Lin</u>, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang and Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2023<br>
								[<a href="https://github.com/showlab/cosmo">project</a>]
								[<a href="https://arxiv.org/pdf/2401.00849.pdf">paper</a>]
								[<a href="https://github.com/showlab/cosmo">code</a>]
								[<a href="https://huggingface.co/datasets/Awiny/Howto-Interlink7M">dataset</a>]
								[<a href="https://fingerrec.github.io/cosmoe/">MoE</a>]
								<br>
								<a style="color:#FA8072">Interleaved vision-text pretraining, with contrastive and generative modeling, MoE scaling.</a><br>
								<br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/assistgpt.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2306.08640"><b>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</b></a> <br>
							Difei Gao, Lei Ji, Luowei Zhou, <u>Kevin QH. Lin</u>, Joya Chen, Zihan Fan, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2023<br>
								[<a href="https://showlab.github.io/assistgpt/">project</a>]
								[<a href="https://arxiv.org/pdf/2306.08640.pdf">paper</a>]
								<br>
								<a style="color:#FA8072">A video agent for general video understanding.</a><br>

						</td>
					</tr>	
				</tbody>
			</table>
			</ul>

			<h2>Publications</h2>
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="260">
							<img src="figures/videollm-online.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://openaccess.thecvf.com//content/CVPR2024/papers/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.pdf"><b>VideoLLM-online: Towards Large Video-Language Model for Streaming Video. </b></a> <br>Joya Chen, Zhaoyang Lv, Shiwei Wu, <u>Kevin QH. Lin</u>, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Computer Vision and Pattern Recognition (<b>CVPR</b>), </em>2024.<br> -->
								<em>CVPR, </em>2024<br>
								[<a href="https://showlab.github.io/videollm-online">project</a>]
								[<a href="https://openaccess.thecvf.com//content/CVPR2024/papers/Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.pdf">paper</a>]
								[<a href="https://showlab.github.io/videollm-online">code</a>]
								<br>
								<a style="color:#FA8072">The first streaming video-language model, achieving 10 FPS for long video online processing.</a><br>
								<br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/sf.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2312.01987"><b>Bootstrapping SparseFormers from Vision Foundation Models</b></a> <br>Ziteng Gao, Zhan Tong, <u>Kevin QH. Lin</u>, Joya Chen, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Computer Vision and Pattern Recognition (<b>CVPR</b>), </em>2024.<br> -->
								<em>CVPR, </em>2024<br>
								[<a href="https://arxiv.org/abs/2312.01987.pdf">paper</a>]
								[<a href="https://github.com/showlab/sparseformer">code</a>]
								<br>
								<a style="color:#FA8072">An efficient pathway to transform vision foundation models into SparseFormer.</a><br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/visorgpt.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2305.13777"><b>VisorGPT: Learning Visual Prior via Generative Pre-Training</b></a> <br>
							Jinheng Xie, Kai Ye, Yudong Li, Yuexiang Li, <u>Kevin QH. Lin</u>, Yefeng Zheng, Linlin Shen, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Neural Information Processing Systems (<b>NeurIPS</b>), </em>2023.<br> -->
								<em>NeurIPS, </em>2023<br>
								[<a href="https://sierkinhane.github.io/visor-gpt/">project</a>]
								[<a href="https://arxiv.org/pdf/2305.13777.pdf">paper</a>]
								[<a href="https://github.com/Sierkinhane/VisorGPT">code</a>]
								<br>
							<a style="color:#FA8072">Model visual prior by language modeling.</a><br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/univtg.jpg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2307.16715"><b>UniVTG: Towards Unified Video-Language Temporal Grounding</b></a> <br>
							<u>Kevin QH. Lin</u>, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex JP. Wang, Rui Yan, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>International Conference on Computer Vision (<b>ICCV</b>), </em>2023.<br> -->
								<em>ICCV, </em>2023<br>
								[<a href="https://huggingface.co/spaces/KevinQHLin/UniVTG">demo</a>]
								[<a href="https://arxiv.org/pdf/2307.16715.pdf">paper</a>]
								[<a href="https://github.com/showlab/UniVTG">code</a>]
								<br>
							<!-- <font color="#FF0000">The first video temporal grounding pretraining model, unifying diverse temporal annotations to power moment retrieval, highlight detection and video summarization.</font> -->
							<a style="color:#FA8072">The first video temporal grounding pretraining model, unifying diverse temporal labels.</a><br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/egovlpv2.jpeg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2307.05463"><b>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</b></a> <br>
							Shraman Pramanick, Yale Song, Sayan Nag, <u>Kevin QH. Lin</u>, Hardik Shah, Mike Z. Shou, Rama Chellappa, Pengchuan Zhang<br>
							<p style="margin-top:3px">
								<!-- <em>International Conference on Computer Vision (<b>ICCV</b>), </em>2023.<br> -->
								<em>ICCV, </em>2023<br>
								[<a href="https://shramanpramanick.github.io/EgoVLPv2/">project</a>]
								[<a href="https://arxiv.org/pdf/2307.05463.pdf">paper</a>]
								[<a href="https://github.com/facebookresearch/EgoVLPv2">code</a>]
								<br>
							<a style="color:#FA8072">The new generation of egocentric video-language pre-training.</a><br>							
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/tldr.png" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2305.20087"><b>Too Large; Data Reduction for Vision-Language Pre-Training</b></a> <br>
							Alex JP. Wang, <u>Kevin QH. Lin</u>, David JH. Zhang, Stan WX. Lei, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>International Conference on Computer Vision (<b>ICCV</b>), </em>2023.<br> -->
								<em>ICCV, </em>2023<br>
								[<a href="https://arxiv.org/pdf/2305.20087.pdf">paper</a>]
								[<a href="https://github.com/showlab/data-centric.vlp">code</a>]
								<br>
							<a style="color:#FA8072">Compress large-scale vision-text dataset into a small, high-quality set.</a><br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/afformer.png" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Affordance_Grounding_From_Demonstration_Video_To_Target_Image_CVPR_2023_paper.html"><b>Affordance
									Grounding from Demonstration Video
									to Target Image</b></a> <br>
							Joya Chen, Difei Gao, <u>Kevin QH. Lin</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Computer Vision and Pattern Recognition (<b>CVPR</b>), </em>2023.<br> -->
								<em>CVPR, </em>2023<br>
								[<a href="https://arxiv.org/pdf/2303.14644.pdf">paper</a>]
								[<a href="https://github.com/showlab/afformer">code</a>]
								<br>
								<a style="color:#FA8072">Learning where to interact (affordance) from demonstration videos.</a><br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/aio.jpg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.html"><b>All
									in one: Exploring unified video-language
									pre-training</b></a> <br>
							Alex JP. Wang, Yixiao Ge, Rui Yan, Yuying Ge, <u>Kevin QH. Lin</u>, Satoshi Tsutsui, Xudong
							Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, Mike Z. Shou.
							<p style="margin-top:3px">
								<!-- <em>Computer Vision and Pattern Recognition (<b>CVPR</b>), </em>2023.<br> -->
								<em>CVPR, </em>2023<br>
								[<a
									href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf">paper</a>]
								[<a href="https://github.com/showlab/all-in-one">code</a>]
								<br>
								<a style="color:#FA8072">The first unified video-language pretrained model.</a><br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="EgoVLP/static/images/framework.jpeg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<!-- <td><a href="https://arxiv.org/abs/2206.01670"><b>Egocentric Video-Language Pretraining</b></a> -->
						<td><a href="https://arxiv.org/abs/2206.01670"><b>Egocentric Video-Language Pretraining</b></a>
							<br>
							<u>Kevin QH. Lin</u>, Alex JP. Wang, M. Soldan, M. Wray, R. Yan, Eric ZC. Xu, D. Gao, R. Tu,
							W. Zhao, W. Kong, C. Cai, H. Wang, D. Damen, B. Ghanem, W. Liu, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Neural Information Processing Systems (<b>NeurIPS</b>), </em>2022. <font
									color="#FF0000"><b>Spotlight (1.7%)</b></font><br> -->
								<em>NeurIPS, </em>2022. <a style="color:#FA8072">Spotlight (1.7%)</a><br>
								[<a href="https://qinghonglin.github.io/EgoVLP/">project</a>]
								[<a href="https://arxiv.org/pdf/2206.01670.pdf">paper</a>]
								[<a href="https://github.com/showlab/EgoVLP">code</a>]
								[<a href="EgoVLP/static/images/poster.pdf">poster</a>]
								<br>
								<!-- <font color="#FF0000">The first egocentric vision-language pretrained model.<br> -->
								<a style="color:#FA8072">The first egocentric vision-language pretrained model. </a><br>
								<a style="color:#FA8072">EgoVis 2022/2023 Distinguished Paper Award & PREMIA Best Student Paper Award 2023.</a><br>
								<a style="color:#FA8072">Double champions in Ego4D & Epic-Kitchens CVPR 2022 challenges.</a><br>
									<!-- Double champions in Ego4D & Epic-Kitchens CVPR 2022 challenges.</font> <a href=https://cde.nus.edu.sg/ece/news-detail/double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges/>[News]</a> -->
						</td>
					</tr>
					<tr></tr>
					<tr></tr>
				</tbody>
			</table>
			</ul>

			<h2>Projects</h2>
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="260">
							<img src="figures/vlog.jpeg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://github.com/showlab/VLog"><b>VLog: Video as a Long Document</b></a> <br>
							<p style="margin-top:3px">
								[<a href="https://huggingface.co/spaces/TencentARC/VLog">demo</a>]
								[<a href="https://github.com/showlab/VLog">code</a>]
								[<a href="https://twitter.com/KevinQHLin/status/1649124447037841408">twitter</a>]
								<br>
								Given a long video, we turn it into a doc containing visual + audio info. By sending this doc to ChatGPT, we can chat over the video!
						</td>
					</tr>
					<tr></tr>
					<tr></tr>
				</tbody>
			</table>
			</ul>

			<h2>Honors</h2>
			<ul>
				<li>
					<div style="float:left; text-align:left">Egocentric Vision (EgoVis) Distinguished Paper Award</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">CVPR Outstanding Reviewers</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">PREMIA Best Student Paper Awards, Gold Award</div>
					<div style="float:right; text-align:right">2023</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Show Lab Annual Award</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">NeurIPS Scholar Award</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<!-- <div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship <a href=https://mp.weixin.qq.com/s/06e7m8twMen9DzrAE0raPA>[News]</a></div> -->
					<div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship, Second Prize</a></div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">1st Place on Ego4D - Object State Change ClassiÔ¨Åcation
						Challenge, CVPR</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left"></div>1st Place on EPIC-Kitchens - Multi-Instance Retrieval
					Challenge, CVPR<div style="float:right; text-align:right">2022</div>
				</li>
				<!--
	<li>
		<div style="float:left; text-align:left">2nd Place on Ego4D - Natural Language Queries Challenge, CVPR</div> <div style="float:right; text-align:right">2022</div>
	</li>
-->
				<!-- <li>
					<div style="float:left; text-align:left">AAAI Student Scholarship</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">SIGIR Student Travel Grant</div>
					<div style="float:right; text-align:right">2021</div>
				</li> -->
				<!--	<li>
		<div style="float:left; text-align:left">Outstanding Graduate at SZU</div> <div style="float:right; text-align:right">2022, 2019</div>
	</li>
-->
				<li>
					<div style="float:left; text-align:left">China National Scholarship</div>
					<div style="float:right; text-align:right">2018, 2021</div>
				</li>
			</ul>


			<h2>Service</h2>
			<ul>
				<li>
					<!-- SDM -->
					<p>Conference Reviewer: CVPR, ICCV, ECCV, ICML, NeurIPS, KDD, EMNLP, AAAI, IJCAI, ICME, ICASSP, ACM MM.</p> 
				</li>
				<li>
					<p>Journal Reviewer: TPAMI, IJCV, TNNLS, TMM, Neurocomputing, Pattern Recognition, etc.</p>
				</li>
				<li>
					<p>Co-organizer of <a href="https://theaitalks.org/">The AI Talks.</a>
				</li>
			</ul>

       <!-- </tbody></table>
			<h2>Acknowledgment</h2>
			I have been fortunate to work with these wonderful people who generously provided me with mentorship.
        	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>  
              <td style="padding:1%;width:20%;vertical-align:top">
                <p align="">
                <img style="width:100%" align="center" src="figures/tencent.jpg" class="hoverZoomLink">
                <p align="center">
                <a href="https://www.tencent.com/">@ Tencent</a>
                <br>
                <p align="left">
                <a href="https://scholar.google.com/citations?user=AjxoEpIAAAAJ&hl=zh-CN">Wei Liu</a>
                <br>
                </p>
                </td>

             <td style="padding:1%;width:20%;vertical-align:top">
                <p align="center">
                <img style="width:100%" align="center" src="figures/bristol.jpg" class="hoverZoomLink">
                <p align="center">
                <a href="https://ai.facebook.com/">@ University of Bristol</a>
                <br>
                <p align="left">
                <a href="https://dimadamen.github.io/">Dima Damen</a>
                <br>
                <a href="https://mwray.github.io/">Michael Wray</a>
                <br>
                </p>
                </td>          
    
             <td style="padding:1%;width:20%;vertical-align:top">
                <p align="center">
                <img style="width:100%" align="center" src="figures/kaust.png" class="hoverZoomLink">
                <p align="center">
               	<br>
                <a href="https://www.kaust.edu.sa/en/">@ KAUST</a>
                <p align="left">
                <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>
                <br>
                <a href="https://www.mattiasoldan.com/">Mattia Soldan</a>
                <br>
                </p>
                </td>

             <td style="padding:1%;width:20%;vertical-align:top">
                <p align="center">
                <img style="width:100%" align="center" src="figures/meta.jpg" class="hoverZoomLink">
                <p align="center">
                <a href="https://ai.facebook.com/">@ Meta AI</a>
                <br>
                <p align="left">
                <a href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang</a>
                <br>
                <a href="https://xidexia.github.io/">Xide Xia</a>
                <br>
             </td>

             <td style="padding:1%;width:20%;vertical-align:top">
                <p align="center">
                <img style="width:50%" align="center" src="figures/realitylab.png" class="hoverZoomLink">
                <p align="center">
                <a href="https://about.meta.com/realitylabs/">@ Meta Reality Labs</a>
                <br>
                <p align="left">
                <a href="https://lvzhaoyang.github.io/">Zhaoyang Lv</a>
                <br>
               <a href="https://scholar.google.com/citations?user=8dfruq4AAAAJ&hl=en">Lambert Mathias</a>
                <br>
             </td>

           <td> </td>
           <td> </td>
           <td> </td>
          </tbody></table> 

        	<tr>  
              <td style="padding:1%;width:20%;vertical-align:top">
                <p align="">
                <img style="width:20%" align="center" src="figures/ms.webp" class="hoverZoomLink">
                <p align="left">
                <a href="https://www.microsoft.com/en-us/research/">@ Microsoft Gen AI</a>
                <br>
                <p align="left">
                <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li</a>
                <br>
                </p>
                </td>
            </tr>
          </tbody></table> 
 -->
			</br>
			<div align="center">
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/count2/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a></div> -->
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/map/yZuS/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a></div> -->
				<a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/mini/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>
		</div>
		</br>

		<div align="center">

			<body>
				<font color="gray">&copy Kevin</font>
			</body>
		</div>

		<script>
			(function (i, s, o, g, r, a, m) {
				i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
					(i[r].q = i[r].q || []).push(arguments)
				}, i[r].l = 1 * new Date(); a = s.createElement(o),
					m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
			})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

			ga('create', 'UA-88615920-1', 'auto');
			ga('send', 'pageview');
		</script>
	</div>
	</div>
</body>

</html>
