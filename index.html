<!DOCTYPE html>
<!-- saved from url=(0025)https://qinghonglin.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="shortcut icon" href="https://qinghonglin.github.io/myIcon.ico">
	<meta name="google-site-verification" content="PcjE-PoDvp7KoKeZ5wE1g_BU8VI5wioTfiAgbIst__4" />
	<meta name="keywords" content="Qinghong Lin">
	<meta name="description" content="Qinghong Lin&#39;s homepage">
	<!-- <link rel="icon" href="icon.ico" type="figures/emoji"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/technologist-light-skin-tone_1f9d1-1f3fb-200d-1f4bb.png"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/snowflake_2744-fe0f.png"> -->
	<link rel="shortcut icon" href="./figures/world.png">
	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<title>Kevin Qinghong Lin @ University of Oxford</title>
	<script async="" src="./index_files/analytics.js"></script>
	<script type="text/javascript" async="" src="./index_files/ga.js"></script>
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-39824124-1']);
		_gaq.push(['_trackPageview']);
		(function () {
			var ga = document.createElement('script');
			ga.type = 'text/javascript';
			ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0];
			s.parentNode.insertBefore(ga, s);
		})();
	</script>
</head>

<body>
	<div id="layout-content" style="margin-top:25px">
		<!-- <table> -->
		<table style="margin-bottom: -20px;">
			<tbody>
				<tr>
					<td width="670">
						<div id="toptitle">

							<!-- <h1>Kevin Qinghong Lin</h1> -->
							<h1><span style="font-family: 'Comic Sans MS', Georgia; color: #6495ED;">Kevin</span>
								Qinghong Lin</h1>
						</div>
						<!-- <h3>Ph.D. Student</h3> -->
						<h3>Postdoctoral Researcher</h3>
						<p>
							<a href="https://torrvision.com/index.html">Torr Vision Group</a><br />
							<a href="https://www.ox.ac.uk/">University of Oxford</a><br />
							<br>
							Email: <u><a href="mailto:kevin.qh.lin@gmail.com">kevin.qh.lin [at] gmail.com</a></u>
						</p>
						<p>
							<!-- <a href="https://scholar.google.com/citations?user=EvbGjlUAAAAJ"><img
									src="./index_files/google_scholar.png" height="30px"></a>
							<a href="https://github.com/QinghongLin"><img src="./index_files/github.png"
									height="30px"></a>
							<a href="https://www.linkedin.com/in/kevinqhlin/"><img src="./index_files/linkedin.png"height="30px"></a>
							<a href="https://twitter.com/KevinQHLin"><img src="./index_files/x.png" height="30px"></a> -->
<a href="https://scholar.google.com/citations?user=EvbGjlUAAAAJ">[Scholar]</a>
<a href="https://github.com/QinghongLin">[Github]</a>
<a href="https://huggingface.co/KevinQHLin">[HF]</a>
<a href="https://www.linkedin.com/in/kevinqhlin/">[LinkedIn]</a>
<a href="https://twitter.com/KevinQHLin">[Twitter]</a>
						</p>
					</td>
					<td>
						<img src="./figures/kevin_new.jpeg" border="0" width="250"><br>
						<!-- <img src="./figures/kevin.jpeg" border="0" width="250"><br> -->
						<!-- <p style="font-size: 10px; font-family: 'Comic Sans MS', Georgia; text-align: left;">Photo taken on <a href="https://en.wikipedia.org/wiki/Rottnest_Island">Rottnest Island</a>.</p> -->
					</td>
				</tr>
				<tr>
				</tr>
			</tbody>
		</table>

		<h2>Biography</h2>
		<!-- <img src="./figures/wordcloud.png" style="float: right; width: 280px; margin: 0 0 10px 10px;">		 -->
		<p>
		</p>
		<div style="text-align:justify">
			I am a Postdoctoral
			Researcher in <a href="https://torrvision.com/index.html">University of Oxford</a>, working with <a href="https://scholar.google.com/citations?user=kPxa2w0AAAAJ">Prof. Philip
				Torr</a>.
			<p></p>
			I obtained my PhD from <a href="https://sites.google.com/view/showlab/home">National University of
				Singapore</a> in three years, luckily advised
			by <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl">Prof. Mike
				Shou</a>.
			<p></p>
			<p></p>
			I was fortunate to intern at Tencent / Meta AI / Meta Reality Labs / Microsoft Research.
			<p></p>
			<!-- I used to work on video understanding. I’m now focus on agentic, visual intelligence.
			<p></p> -->
			<p>
			</p>
			My research focuses on developing multimodal intelligent agents to assist humans. This spans abilities
				like:</p>
			<ul>
				<li>
					See multimodally: video understanding (<a href="https://videomind.github.io/">VideoMind</a>,<a href="https://showlab.github.io/videollm-online/">VideoLLM-online</a>) from scalable human data (<a href="https://arxiv.org/abs/2206.01670">EgoVLP</a>,<a href="https://arxiv.org/abs/2307.16715">UniVTG</a>).
				</li>
				<li>
					Think like humans: adaptive reasoning via reinforcement learning (<a href="https://arxiv.org/abs/2505.16854">Think or Not</a>) and symbolic coding (<a href="https://showlab.github.io/Code2Video/">Code2Video</a>,<a href="https://csu-jpg.github.io/VCode/">VCode</a>).
				<li>
					Act in environments: computer-use agents (<a href="https://github.com/showlab/ShowUI">ShowUI</a>,<a href="">GroundCUA</a>) for human workflows (<a href="https://github.com/Paper2Poster/Paper2Poster">Paper2Poster</a>,<a href="https://showlab.github.io/Paper2Video/">Paper2Video</a>).
				</li>
			</ul>
			</ul>
			<p>I’m open to collaborate with academic / industry / startups. Feel free to drop me an email.</p>
<div style="float: left; text-align: center;">

    <img 
        src="https://github-readme-stats-one-bice.vercel.app/api?username=qinghonglin&show_icons=true&include_all_commits=true&role=OWNER,COLLABORATOR"
        style="max-width: 420px;"
    >

<p style="text-align: left;">
    I am passionate about open-source!
</p>


</div>
<div style="clear: both;">
</div>
			<h2>Selected Publications <a href="https://scholar.google.com/citations?user=EvbGjlUAAAAJ">[Google
					Scholar]</a></h2>
			† indicates equal contribution. <u style="text-decoration-color: #6495ED;">Denotes student I mentored.</u> ✉ indicates corresponding author. 
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="100%">
							<!-- <img src="figures/asmr.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> -->
							<a href="https://arxiv.org/abs/2512.13281"><b>Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</b></a><br>
							<u style="text-decoration-color: #6495ED;">Jiaqi Wang</u>, Weijia Wu, Yi Zhan, Rui Zhao, Ming Hu, James Cheng, Wei Liu, Philip Torr, <u>Kevin QH. Lin✉</u>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="https://video-reality-test.github.io/">project</a>]
								[<a href="https://arxiv.org/pdf/2512.13281">paper</a>]
								[<a href="https://github.com/video-reality-test/video-reality-test">code</a>]
								<br>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/aui.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2511.15567v1"><b>Computer-Use Agents as Judges for Generative User Interface</b></a><br>
							<u>Kevin QH. Lin†</u>, <u style="text-decoration-color: #6495ED;">Siyuan Hu†</u>, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="https://showlab.github.io/AUI/">project</a>]
								[<a href="http://arxiv.org/pdf/2511.15567v1">paper</a>]
								[<a href="https://github.com/showlab/AUI/">code</a>]
								[<a href="https://huggingface.co/spaces/showlab/AUI">demo</a>]
								[<a href="https://x.com/HuggingPapers/status/1993291485446316537">twitter</a>]
								<br>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/vcode.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="http://arxiv.org/abs/2511.02778"><b>VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</b></a><br>
							<u>Kevin QH. Lin†</u>, <u style="text-decoration-color: #6495ED;">Yuhao Zheng†</u>, <u style="text-decoration-color: #6495ED;">Hangyu Ran†</u>, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex JP. Wang.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="https://csu-jpg.github.io/VCode/">project</a>]
								[<a href="http://arxiv.org/pdf/2511.02778">paper</a>]
								[<a href="https://github.com/CSU-JPG/VCode">code</a>]
								[<a href="https://huggingface.co/spaces/CSU-JPG/VCode">demo</a>]
								[<a href="https://x.com/_akhaliq/status/1986878915168592315">twitter</a>]								
								<br>
								<a style="color:#FA8072">#1 Huggingface daily paper.</a>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/paper2video.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2510.05096"><b>Paper2Video: Automatic Video Generation from Scientific Papers</b></a><br>
							<u style="text-decoration-color: #6495ED;">Zeyu Zhu†</u>, <u>Kevin QH. Lin†</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="https://showlab.github.io/Paper2Video/">project</a>]
								[<a href="https://arxiv.org/abs/2510.05096">paper</a>]
								[<a href="https://github.com/showlab/Paper2Video">code</a>]
								[<a href="https://huggingface.co/datasets/ZaynZhu/Paper2Video">dataset</a>]
								[<a href="https://x.com/pitiklinov/status/1994674230051401871">twitter</a>]
								<br>
								<a style="color:#FA8072">#2 Huggingface daily paper.</a></br>											
								<a style="color:#FA8072">1.9K github stars. 1M+ twitter views. Highlighted by <a href="https://news.ycombinator.com/item?id=45553701">YC Hacker News</a></a>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/code2video.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2510.01174"><b>Code2Video: A Code-centric Paradigm for Educational Video Generation</b></a><br>
							<u style="text-decoration-color: #6495ED;">Yanzhe Chen†</u>, <u>Kevin QH. Lin†</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="https://showlab.github.io/Code2Video/">project</a>]
								[<a href="https://arxiv.org/abs/2510.01174">paper</a>]
								[<a href="https://github.com/showlab/Code2Video">code</a>]
								[<a href="https://huggingface.co/datasets/YanzheChen/MMMC">dataset</a>]
								[<a href="https://x.com/KevinQHLin/status/1974199353695941114">twitter</a>]								

								<br>
								<a style="color:#FA8072">1.4K github stars.</a>																
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/paper2poster.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2505.21497"><b>Paper2Poster: Towards Multimodal Poster
									Automation from Scientific Papers</b></a><br>
							<u style="text-decoration-color: #6495ED;">Wei Pang†</u>, <u>Kevin QH. Lin†</u>, Xiangru
							Jian†, Xi He, Philip Torr.<br>
							<p style="margin-top:3px">
								<em>NeurIPS D&B, </em>2025<br>
								<em>ICML MAS workshop, </em>2025. <a style="color:#FA8072">Oral</a><br>
								[<a href="https://paper2poster.github.io/">project</a>]
								[<a href="https://arxiv.org/abs/2505.21497">paper</a>]
								[<a href="https://github.com/Paper2Poster/Paper2Poster">code</a>]
								[<a href="https://huggingface.co/datasets/Paper2Poster/Paper2Poster">datasets</a>]
								[<a href="https://huggingface.co/spaces/camel-ai/Paper2Poster">demo</a>]
								[<a href="https://github.com/Paper2Poster/Paper2Poster/blob/main/assets/teaser.jpeg">poster</a>]
								[<a href="https://x.com/_akhaliq/status/1927721150584390129">twitter</a>]
								<br>
								<a style="color:#FA8072">3K github stars. 1.2K twitter likes.</a>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/ton.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2505.16854"><b>Think or Not? Selective Reasoning via
									Reinforcement Learning for Vision-Language Models</b></a><br>
							<u style="text-decoration-color: #6495ED;">Jiaqi Wang†</u>, <u>Kevin QH. Lin†</u>, James
							Cheng, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>NeurIPS, </em>2025<br>
								[<a href="https://arxiv.org/abs/2505.16854">paper</a>]
								[<a href="https://github.com/kokolerk/TON">code</a>]
								[<a
									href="https://huggingface.co/collections/kolerk/ton-682ad9038395c21e228a645b">huggingface</a>]
								[<a href="https://x.com/_akhaliq/status/1925923967711236268">twitter</a>]
								<br>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/videomind.jpg" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2503.13444"><b>VideoMind: A Chain-of-LoRA Agent for Long
									Video Reasoning</b></a><br>
							Ye Liu†, <u>Kevin QH. Lin†</u>, Chang Wen Chen, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								<em>NeurIPS LAW workshop, </em>2025. <a style="color:#FA8072">Spotlight</a><br>								
								[<a href="https://videomind.github.io/">project</a>]
								[<a href="https://arxiv.org/abs/2503.13444">paper</a>]
								[<a href="https://github.com/yeliudev/VideoMind">code</a>]
								[<a href="https://huggingface.co/datasets/yeliudev/VideoMind-Dataset">dataset</a>]
								[<a href="https://huggingface.co/spaces/yeliudev/VideoMind-2B">demo</a>]
								[<a href="https://x.com/_akhaliq/status/1905658095348359484">twitter</a>]
								<br>
						</td>
					</tr>
					<tr>
						<td width="100%">
<a href="https://arxiv.org/abs/2511.07332"><b>Grounding Computer Use Agents on Human Demonstrations</b></a><br>
Aarash Feizi†, Shravan Nayak†, Xiangru Jian, <u>Kevin QH. Lin</u>, Kaixin Li, Rabiul Awal, Xing Han Lù, Johan Obando-Ceron, Juan A Rodriguez, Nicolas Chapados, David Vazquez, Adriana Romero-Soriano, Reihaneh Rabbany, Perouz Taslakian, Christopher Pal, Spandana Gella, Sai Rajeswar.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="https://groundcua.github.io/">project</a>]
								[<a href="https://arxiv.org/abs/2511.07332">paper</a>]
								[<a href="https://github.com/ServiceNow/GroundCUA">code</a>]
								[<a href="https://huggingface.co/datasets/ServiceNow/GroundCUA">huggingface</a>]
								[<a href="https://x.com/aarashfeizi/status/1988648178204520678">twitter</a>]				
								<br>
								<a style="color:#FA8072">#2 Huggingface daily paper.</a></br>
								<a style="color:#FA8072">The dataset has been downloaded over 150,000 times</a>
						</td>
					</tr>
					<tr>
						<td width="100%">
<a href="https://www.servicenow.com/research/publication/shravan-nayak-ui-v-icml2025.html"><b>UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction</b></a><br>
<u style="text-decoration-color: #6495ED;">Shravan Nayak†</u>, <u style="text-decoration-color: #6495ED;">Xiangru Jian†</u>, <u>Kevin QH. Lin</u>, Juan A Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M Tamer Özsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, Perouz Taslakian, Spandana Gella, Sai Rajeswar.<br>
							<p style="margin-top:3px">
								<em>ICML, </em>2025<br>
								[<a href="https://uivision.github.io/">project</a>]
								[<a href="https://arxiv.org/abs/2408.12528">paper</a>]
								[<a href="https://github.com/uivision/UI-Vision">code</a>]
								[<a href="https://huggingface.co/datasets/ServiceNow/ui-vision">huggingface</a>]
								[<a href="https://x.com/_akhaliq/status/1826822188990615845">twitter</a>]				
								<br>
						</td>
					</tr>
					<tr>
						<td width="100%">
<a href="https://huggingface.co/papers/2408.12528"><b>Show-o: One Single Transformer to Unify Multimodal Understanding and Generation</b></a><br>
Jinheng Xie†, Weijia Mao†, Zechen Bai†, David JH. Zhang†, Weihao Wang, <u>Kevin QH. Lin</u>, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>ICLR, </em>2025<br>
								[<a href="https://showlab.github.io/Show-o/">project</a>]
								[<a href="https://arxiv.org/abs/2408.12528">paper</a>]
								[<a href="https://github.com/showlab/Show-o">code</a>]
								[<a href="https://huggingface.co/showlab/ShowUI-2B">huggingface</a>]
								[<a href="https://huggingface.co/spaces/showlab/Show-o">demo</a>]
								[<a href="https://x.com/_akhaliq/status/1826822188990615845">twitter</a>]								
								<br>
								<a style="color:#FA8072">1.8K github stars.</a><br>
								<a style="color:#FA8072">Most Influential ICLR Papers #4</a><br>
						</td>
					</tr>
					<tr>
						<td width="100%">
<a href="https://dl.acm.org/doi/abs/10.1145/3688865.3689482"><b>AssistGPT: Towards Multi-modal Agent for Human-Centric AI Assistant</b></a><br>
Difei Gao, Siyuan Hu, <u>Kevin QH. Lin</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>ACMMM HCMA workshop, </em>2024. <a style="color:#FA8072">Best Demo Paper</a><br>
								[<a href="https://showlab.github.io/assistgpt/">project</a>]
								[<a href="https://dl.acm.org/doi/abs/10.1145/3688865.3689482">paper</a>]
								[<a href="https://x.com/_akhaliq/status/1669531702442483713">twitter</a>]								
								<br>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/univtg.jpg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2307.16715"><b>VideoLLM-online: Online Video Large Language Model for Streaming Video</b></a> <br>
							Joya Chen, Zhaoyang Lv, Shiwei Wu, <u>Kevin QH. Lin</u>, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>International Conference on Computer Vision (<b>ICCV</b>), </em>2023.<br> -->
								<em>CVPR, </em>2024<br>
								[<a href="https://showlab.github.io/videollm-online/">project</a>]
								[<a href="https://arxiv.org/pdf/2307.16715.pdf">paper</a>]
								[<a href="https://arxiv.org/abs/2408.16730">VideoLLM-MoD</a>]
								[<a href="https://github.com/showlab/videollm-online">code</a>]
								[<a href="https://huggingface.co/datasets/chenjoya/videollm-online-chat-ego4d-134k">dataset</a>]
								[<a href="https://x.com/arankomatsuzaki/status/1802900275813883999">twitter</a>]								
								<br>
								<a style="color:#FA8072">600+ github stars.</a><br>
					</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/vla.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://huggingface.co/papers/2411.17465"><b>ShowUI: One Vision-Language-Action
									Model for GUI Visual Agent</b></a><br>
							<u>Kevin QH. Lin</u>, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan WX.
							Lei, Lijuan Wang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>CVPR, </em>2025<br>
								<em>NeurIPS OWA workshop, </em>2024. <a style="color:#FA8072">Oral</a><br>
								[<a href="https://arxiv.org/abs/2411.17465">paper</a>]
								[<a href="https://github.com/showlab/ShowUI">code</a>]
								[<a href="https://huggingface.co/showlab/ShowUI-2B">huggingface</a>]
								[<a href="https://huggingface.co/datasets/showlab/ShowUI-desktop-8K">dataset</a>]
								[<a href="https://huggingface.co/spaces/showlab/ShowUI">demo</a>]
								[<a href="https://x.com/_akhaliq/status/1864387028856537400">twitter</a>]								
								<br>
								<a style="color:#FA8072">#1 Huggingface daily paper.</a><br>
								<a style="color:#FA8072">Outstanding Paper Award, NeurIPS Open-World Agents Workshop
									2024.</a><br>
								<a style="color:#FA8072">The model has been downloaded for over 240,000 times. 1.6K github stars.</a>
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/vlog.jpg" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="http://arxiv.org/abs/2503.09402"><b>VLog: Video-Language Models by Generative
									Retrieval of Narration Vocabulary</b></a><br>
							<u>Kevin QH. Lin</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>NeurIPS OWA workshop, </em>2024. <a style="color:#FA8072">Oral</a><br> -->
								<em>CVPR, </em>2025<br>
								[<a href="http://arxiv.org/abs/2503.09402">paper</a>]
								[<a href="https://github.com/showlab/VLog">code</a>]
								[<a href="https://x.com/KevinQHLin/status/1649124447037841408">twitter</a>]								
								<br>
								<a style="color:#FA8072">580+ github stars.</a><br>								
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/videogui.jpg" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://showlab.github.io/VideoGUI/"><b>VideoGUI: A Benchmark for GUI Automation
									from Instructional Videos</b></a><br>
							<u>Kevin QH. Lin</u>, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan
							Wang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>NeurIPS D&B, </em>2024. <a style="color:#FA8072">Spotlight</a><br>
								[<a href="https://showlab.github.io/VideoGUI/">project</a>]
								[<a href="https://showlab.github.io/VideoGUI/assets/preprint.pdf">paper</a>]
								[<a href="https://github.com/showlab/VideoGUI">code</a>]
								[<a href="https://x.com/arankomatsuzaki/status/1802516425011024071">twitter</a>]																
								<br>
								<!-- <a style="color:#FA8072">Can an agent recreate PowerPoint animation effects from instructional videos?</a><br> -->

						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/movieseq.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2407.21757"><b>Learning Video Context as Interleaved
									Multimodal Sequences </b></a> <br><u>Kevin QH. Lin</u>, Pengchuan Zhang, Difei Gao,
							Xide Xia, Joya Chen, Ziteng Gao, Jinheng Xie, Xuhong Xiao, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>ECCV, </em>2024<br>
								[<a href="https://arxiv.org/abs/2407.21757">paper</a>]
								[<a href="https://github.com/showlab/MovieSeq">code</a>]
								<br>
								<!-- <a style="color:#FA8072">Video in-context learning using interleaved sequences of images, videos, plots and dialogues.</a><br> -->
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="figures/univtg.jpg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td> --><a href="https://arxiv.org/abs/2307.16715"><b>UniVTG: Towards Unified Video-Language
									Temporal Grounding</b></a> <br>
							<u>Kevin QH. Lin</u>, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex JP.
							Wang, Rui Yan, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>International Conference on Computer Vision (<b>ICCV</b>), </em>2023.<br> -->
								<em>ICCV, </em>2023<br>
								[<a href="https://arxiv.org/pdf/2307.16715.pdf">paper</a>]
								[<a href="https://github.com/showlab/UniVTG">code</a>]
								[<a href="https://huggingface.co/spaces/KevinQHLin/UniVTG">demo</a>]
								[<a href="https://x.com/_akhaliq/status/1686223119718006784">twitter</a>]								
								<br>
								<a style="color:#FA8072">370+ github stars.</a><br>																
								<!-- <font color="#FF0000">The first video temporal grounding pretraining model, unifying diverse temporal annotations to power moment retrieval, highlight detection and video summarization.</font> -->
								<!-- <a style="color:#FA8072">The first video temporal grounding pretraining model, unifying diverse temporal labels.</a><br> -->
						</td>
					</tr>
					<tr>
						<td width="100%">
							<!-- <img src="EgoVLP/static/images/framework.jpeg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td> -->
						<!-- <td><a href="https://arxiv.org/abs/2206.01670"><b>Egocentric Video-Language Pretraining</b></a> -->
						<!-- <td> -->
							<a href="https://arxiv.org/abs/2206.01670"><b>Egocentric Video-Language Pretraining</b></a>
							<br>
							<u>Kevin QH. Lin</u>, Alex JP. Wang, M. Soldan, M. Wray, R. Yan, Eric ZC. Xu, D. Gao, R. Tu,
							W. Zhao, W. Kong, C. Cai, H. Wang, D. Damen, B. Ghanema, W. Liu, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Neural Information Processing Systems (<b>NeurIPS</b>), </em>2022. <font
									color="#FF0000"><b>Spotlight (1.7%)</b></font><br> -->
								<em>NeurIPS, </em>2022. <a style="color:#FA8072">Spotlight (1.7%)</a><br>
								[<a href="https://qinghonglin.github.io/EgoVLP/">project</a>]
								[<a href="https://arxiv.org/pdf/2206.01670.pdf">paper</a>]
								[<a href="https://arxiv.org/abs/2307.05463">EgoVLPv2</a>]
								[<a href="https://github.com/showlab/EgoVLP">code</a>]
								[<a href="EgoVLP/static/images/poster.pdf">poster</a>]
								[<a href="https://x.com/_akhaliq/status/1533621621038129153">twitter</a>]
								[<a
									href="https://cde.nus.edu.sg/ece/news-detail/double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges/">media</a>]
								<br>
								<!-- <font color="#FF0000">The first egocentric vision-language pretrained model.<br> -->
								<!-- <a style="color:#FA8072">The first egocentric vision-language pretrained model. </a><br> -->
								<a style="color:#FA8072">EgoVis Distinguished Paper Award.</a><br>
								<a style="color:#FA8072">PREMIA Best Student Paper
									Award, Gold Award.</a><br>
								<a style="color:#FA8072">Double champions in Ego4D & Epic-Kitchens CVPR 2022
									challenges.</a><br>
								<!-- Double champions in Ego4D & Epic-Kitchens CVPR 2022 challenges.</font> <a href=https://cde.nus.edu.sg/ece/news-detail/double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges/>[News]</a> -->
						</td>
					</tr>
					<tr></tr>
					<tr></tr>
				</tbody>
			</table>
			</ul>

			<h2>Honors</h2>
			<ul>
				<li>
					<div style="float:left; text-align:left">Tinker Research Grant, Thinking Machines Lab</div>
					<div style="float:right; text-align:right">2025</div>
				</li>
				<li>
					<div style="float:left; text-align:left">DAAD AINeT Fellowship</div>
					<div style="float:right; text-align:right">2025</div>
				</li>
				<li>
					<div style="float:left; text-align:left">CVPR Doctoral Consortium</div>
					<div style="float:right; text-align:right">2025</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Outstanding Paper Award, NeurIPS Open-World Agents</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">NeurIPS Top Reviewers</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Best Demo Paper Award, ACM Multimedia HCMA</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Egocentric Vision (EgoVis) Distinguished Paper Award</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">CVPR Outstanding Reviewers (Top 2%)</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">PREMIA Best Student Paper Awards, Gold Award</div>
					<div style="float:right; text-align:right">2023</div>
				</li>
				<li>
					<div style="float:left; text-align:left">NeurIPS Scholar Award</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<!-- <div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship <a href=https://mp.weixin.qq.com/s/06e7m8twMen9DzrAE0raPA>[News]</a></div> -->
					<div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship, Second Prize</a>
					</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">1st Place on Ego4D - Object State Change Classiﬁcation
						Challenge, CVPR</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left"></div>1st Place on EPIC-Kitchens - Multi-Instance Retrieval
					Challenge, CVPR<div style="float:right; text-align:right">2022</div>
				</li>
				<!--
	<li>
		<div style="float:left; text-align:left">2nd Place on Ego4D - Natural Language Queries Challenge, CVPR</div> <div style="float:right; text-align:right">2022</div>
	</li>
-->
				<!-- <li>
					<div style="float:left; text-align:left">AAAI Student Scholarship</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">SIGIR Student Travel Grant</div>
					<div style="float:right; text-align:right">2021</div>
				</li> -->
				<!--	<li>
		<div style="float:left; text-align:left">Outstanding Graduate at SZU</div> <div style="float:right; text-align:right">2022, 2019</div>
	</li>
-->
				<li>
					<div style="float:left; text-align:left">Show Lab Annual Award</div>
					<div style="float:right; text-align:right">2022, 2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">China National Scholarship</div>
					<div style="float:right; text-align:right">2018, 2021</div>
				</li>
			</ul>


			<h2>Service</h2>
			<ul>
				<li>
					<p>Area Chair: NeurIPS 2025.
				</li>
				<li>
					<p>Workshop Organizer: <a href="https://showlab.github.io/omg/">Open Multimodal Gathering @ NUS</a>;
						<a href="https://sites.google.com/view/loveucvpr25/home">Multimodal Video Agent @ CVPR 25</a>.
				</li>
				<li>
					<!-- SDM -->
					<p>Conference Reviewer: CVPR (2024 Outstanding Reviewers), ICCV, ECCV, NeurIPS (2024 Top Reviewers),
						ICML, ICLR, etc.</p>
				</li>
				<li>
					<p>Journal Reviewer: TPAMI, IJCV, TMLR, TNNLS, TMM, etc.</p>
				</li>
				<li>
					<p>Teaching Assistant:
						<!-- 						<a href="https://nusmods.com/courses/EE6934/deep-learning-advanced">EE6934 Deep Learning</a>, 
						<a href="https://nusmods.com/courses/EE6733/advanced-topics-on-vision-and-machine-learning">EE6733 Advanced Topics on Vision and Machine Learning</a>,
						<a href="https://nusmods.com/courses/EE6733/advanced-topics-on-vision-and-machine-learning">EE4212 Computer Vision</a>	 -->
						<a href="https://nusmods.com/courses/EE6934/deep-learning-advanced">EE6934</a>,
						<a
							href="https://nusmods.com/courses/EE6733/advanced-topics-on-vision-and-machine-learning">EE6733</a>,
						<a
							href="https://nusmods.com/courses/EE6733/advanced-topics-on-vision-and-machine-learning">EE4212</a>
					</p>
				</li>
				<li>
					<p>Co-organizer of <a href="https://theaitalks.org/">The AI Talks.</a>
				</li>
			</ul>

			</br>
			<div align="center">
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/count2/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a></div> -->
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/map/yZuS/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a></div> -->
				<a href="https://info.flagcounter.com/yZuS"><img
						src="https://s05.flagcounter.com/mini/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/"
						alt="Flag Counter" border="0"></a>
			</div>
			</br>

			<div align="center">

				<body>
					<font color="gray">&copy Kevin</font>
				</body>
			</div>

			<script>
				(function (i, s, o, g, r, a, m) {
					i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
						(i[r].q = i[r].q || []).push(arguments)
					}, i[r].l = 1 * new Date(); a = s.createElement(o),
						m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
				})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

				ga('create', 'UA-88615920-1', 'auto');
				ga('send', 'pageview');
			</script>
		</div>
	</div>
</body>

</html>
