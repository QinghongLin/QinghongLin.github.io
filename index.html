<!DOCTYPE html>
<!-- saved from url=(0025)https://qinghonglin.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="shortcut icon" href="https://qinghonglin.github.io/myIcon.ico">
	<meta name="google-site-verification" content="PcjE-PoDvp7KoKeZ5wE1g_BU8VI5wioTfiAgbIst__4" />
	<meta name="keywords" content="Qinghong Lin">
	<meta name="description" content="Qinghong Lin&#39;s homepage">
	<!-- <link rel="icon" href="icon.ico" type="figures/emoji"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/technologist-light-skin-tone_1f9d1-1f3fb-200d-1f4bb.png"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/snowflake_2744-fe0f.png"> -->
	<link rel="shortcut icon" href="./figures/world.png">
	<link rel="stylesheet" href="./index_files/jemdoc.css" type="text/css">
	<title>Qinghong Lin @ National University of Singapore</title>
	<script async="" src="./index_files/analytics.js"></script>
	<script type="text/javascript" async="" src="./index_files/ga.js"></script>
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-39824124-1']);
		_gaq.push(['_trackPageview']);
		(function () {
			var ga = document.createElement('script');
			ga.type = 'text/javascript';
			ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0];
			s.parentNode.insertBefore(ga, s);
		})();
	</script>
</head>

<body>
	<div id="layout-content" style="margin-top:25px">
		<!-- <table> -->
		<table style="margin-bottom: -20px;">
			<tbody>
				<tr>
					<td width="670">
						<div id="toptitle">

							<!-- <h1>Kevin Qinghong Lin</h1> -->
							<h1><span style="font-family: 'Comic Sans MS', Georgia; color: #6495ED;">Kevin</span> Qinghong Lin</h1>
						</div>
						<h3>Ph.D. Student</h3>
						<p>
							<a href="https://sites.google.com/view/showlab">Show Lab</a><br />
							<a href="https://www.nus.edu.sg/">National University of Singapore</a><br />
							<br>
							Email: <u><a href="mailto:kevin.qh.lin@gmail.com">kevin.qh.lin [at] gmail.com</a></u>
						</p>
						<p>
							<a href="https://scholar.google.com/citations?user=EvbGjlUAAAAJ"><img
									src="./index_files/google_scholar.png" height="30px"></a>
							<a href="https://github.com/QinghongLin"><img src="./index_files/github.png"
																height="30px"></a>
							<!-- <a href="https://www.linkedin.com/in/lqh/"><img src="./index_files/linkedin.png"
									height="30px"></a> -->
							<a href="https://twitter.com/KevinQHLin"><img src="./index_files/x.png"
									height="30px"></a>
						</p>
					</td>
					<td>
						<img src="./figures/kevin.jpeg" border="0" width="250"><br>
						<!-- <img src="./figures/kevin.jpeg" border="0" width="250"><br> -->
						<!-- <p style="font-size: 10px; font-family: 'Comic Sans MS', Georgia; text-align: left;">Photo taken on <a href="https://en.wikipedia.org/wiki/Rottnest_Island">Rottnest Island</a>.</p> -->
					</td>
				</tr>
				<tr>
				</tr>
			</tbody>
		</table>

		<h2>Biography</h2>
		<p>
		</p>
		<div style="text-align:justify">
			I am a Ph.D. student (since 2022.8) in <a href="https://sites.google.com/view/showlab">Show Lab @ NUS</a>, working with <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl">Prof. Mike
				Shou</a>.
			<p></p>
			<!-- I have been interned at <a href="https://arxiv.org/abs/2407.21757">Tencent</a>, <a href="https://arxiv.org/abs/2407.21757">Meta AI</a>, <a href="https://arxiv.org/abs/2407.21757">Meta Reality Labs</a>, <a href="https://arxiv.org/abs/2407.21757">Microsoft</a>. -->
			<p></p>
			<!-- <p>I focus on <a style="color:#224B8D;">Video Understanding</a> and <a style="color:#224B8D;">Language Models</a>, aiming to develop assistants to streamline human tasks.</p> -->
			<!-- <p>I work on <a style="color:#224B8D;">Video Understanding</a> -->
			<!-- aiming to develop assistants for human tasks.</p> -->
			<!-- I enjoy building multi-modal models that assist humans.</p>,  -->
			<p>I work on <a style="color:#224B8D;">Vision + Language</a> and <a style="color:#224B8D;">Video Understanding</a>,
			including:</p>
			<ul>
			<li>Vision-Language Pretraining: <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/31fb284a0aaaad837d2930a610cd5e50-Paper-Conference.pdf">EgoVLP</a>,  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf">All-in-One</a></li>			
			<li>Large Multimodal Models: <a href="https://showlab.github.io/Show-o/">Show-o</a>,<a href="https://showlab.github.io/videollm-online/">VideoLLM-online</a></li>
			<li>Multimodal Agents: <a href="https://huggingface.co/showlab/ShowUI-2B">ShowUI</a>, <a href="https://arxiv.org/pdf/2306.08640">AssistGPT</a></li>
			<li>Video Applications: <a href="https://arxiv.org/pdf/2307.16715">UniVTG</a>, <a href="https://github.com/showlab/VLog/">VLog</a></li>
			</ul>
			<p>I am open to discussion and collaboration. Feel free to reach out.</p>
			<h2>News</h2>
				<ul>
				<li>
				    2024 Dec: <a href="https://arxiv.org/abs/2411.17465">ShowUI</a> (<a style="color:#FA8072">Oral</a>) received <a style="color:#FA8072"><u>Outstanding Paper Award</u></a> by <a href="https://sites.google.com/view/open-world-agents/home">NeurIPS Open-World Agents workshop 2024</a>.
				    <!-- 2024 Dec: Try our <a href="https://huggingface.co/spaces/showlab/ShowUI">🤗 ShowUI</a>, an interesting GUI agent in your screenshots! -->
				</li>
				<li>
					2024 Nov: Recognized as <a href="https://neurips.cc/Conferences/2024/ProgramCommittee#top-reviewers" style="color:#FA8072;">NeurIPS 2024 Top Reviewers</a>.
				</li>
				<li>
				    2024 Sept: <a href="https://showlab.github.io/videogui/">VideoGUI</a> (<a style="color:#FA8072">Spotlight</a>), <a href="https://arxiv.org/abs/2408.16730">VideoLLM-MoD</a> got accepted by <a href="https://neurips.cc/">NeurIPS 2024</a>.
				</li>
				<li>
				    2024 Aug: <a href="https://dl.acm.org/doi/abs/10.1145/3688865.3689482">AssistGPT</a> got accepted by <a href="https://hcma2024.github.io/">HCMA@ACM MM 2024</a> as <a style="color:#FA8072"><u>Best Demo Paper</u></a>.
				</li>
				<li>
				    2024 July: <a href="https://arxiv.org/abs/2407.21757">MovieSeq</a> got accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
				</li>
				<!-- <li>
				    2024 Jun: Check out our new work on GUI automation: <a href="https://showlab.github.io/videogui/">VideoGUI</a> and <a href="https://showlab.github.io/GUI-Narrator/">GUI Narrator</a>.
				</li> -->
				<li>
				    2024 Jun: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> received <a href="https://egovis.github.io/awards/2022_2023/" style="color:#FA8072;"><u>Egocentric Vision (EgoVis) Distinguished Paper Award</u></a>.
				</li>
				<li>
					2024 May: Recognized as <a href="https://x.com/CVPR/status/1793616950314369239" style="color:#FA8072;">CVPR 2024 Outstanding Reviewers</a>.
				</li>
				<li>
					2024 Feb: <a href="https://openaccess.thecvf.com//content/CVPR2024/papers;Chen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.pdf">VideoLLM-online</a>, <a href="https://arxiv.org/abs/2312.01987">SparseFormer</a> got accepted by <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
				</li>
				<li>
					2023 Sept: <a href="https://arxiv.org/abs/2305.13777">VisorGPT</a> got accepted by <a href="https://nips.cc/">NeurIPS 2023</a>.
				</li>
				<li>
					2023 Aug: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> received <a href="http://www.premiasg.org/", style="color:#FA8072;"><u>PREMIA Best Student Paper Award (Gold award)</u></a>.
				</li>
				<li>
					2023 July: <a href="https://arxiv.org/abs/2307.16715">UniVTG</a>, <a href="https://arxiv.org/abs/2307.05463">EgoVLPv2</a>, <a href="https://arxiv.org/abs/2305.20087">TL;DR</a> got accepted by <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>.
				</li>
				<li>
					2023 Mar: <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.html">All-in-one</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Affordance_Grounding_From_Demonstration_Video_To_Target_Image_CVPR_2023_paper.html">Afformer</a> got accepted by <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
				</li>
				<li>
					2022 Sept: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> (<a style="color:#FA8072">Spotlight</a>) got accepted by <a href="https://nips.cc/">NeurIPS 2022</a>.
				</li>
				<li>
					2022 Aug: Joined <a href="https://sites.google.com/view/showlab">Show Lab @ NUS</a> to start my Ph.D. journey!
				</li>
					<li>
					2022 Jun: <a href="https://arxiv.org/abs/2206.01670">EgoVLP</a> won <a style="color:#FA8072">Double Champions</a> of 
					<a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">Joint 1st
						Ego4D and 10th EPIC Workshop, CVPR 2022</a>. <a
						href="https://cde.nus.edu.sg/ece/news/double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges/">[News]</a>
				</li>
			</ul>

			<!-- <h2>Preprints</h2>
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="260">
							<img src="figures/cosmo.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2401.00849"><b>CosMo: Contrastive Streamlined Multimodal Model With Interleaved Pre-Training</b></a> <br>
							Alex JP. Wang, Linjie Li, <u>Kevin QH. Lin</u>, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang and Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2023<br>
								[<a href="https://github.com/showlab/cosmo">project</a>]
								[<a href="https://arxiv.org/pdf/2401.00849.pdf">paper</a>]
								[<a href="https://github.com/showlab/cosmo">code</a>]
								[<a href="https://huggingface.co/datasets/Awiny/Howto-Interlink7M">dataset</a>]
								[<a href="https://fingerrec.github.io/cosmoe/">MoE</a>]
								<br>
								<a style="color:#FA8072">Interleaved vision-text pretraining, with contrastive and generative modeling, MoE scaling.</a><br>
								<br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/assistgpt.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2306.08640"><b>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn</b></a> <br>
							Difei Gao, Lei Ji, Luowei Zhou, <u>Kevin QH. Lin</u>, Joya Chen, Zihan Fan, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2023<br>
								[<a href="https://showlab.github.io/assistgpt/">project</a>]
								[<a href="https://arxiv.org/pdf/2306.08640.pdf">paper</a>]
								<br>
								<a style="color:#FA8072">A video agent for general video understanding.</a><br>

						</td>
					</tr>	
				</tbody>
			</table>
			</ul> -->

			<h2>Publications <a href="https://scholar.google.com/citations?user=EvbGjlUAAAAJ">Google Scholar</a></h2>
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="260">
							<img src="figures/vla.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://huggingface.co/papers/2411.17465"><b>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</b></a><br>
								<u>Kevin QH. Lin</u>, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan WX. Lei, Lijuan Wang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>NeurIPS OWA workshop, </em>2024. <a style="color:#FA8072">Oral</a><br>
								[<a href="https://huggingface.co/showlab/ShowUI-2B">huggingface model</a>]
								[<a href="https://arxiv.org/abs/2411.17465">paper</a>]
								[<a href="https://github.com/showlab/ShowUI">code</a>]								
								[<a href="https://huggingface.co/datasets/showlab/ShowUI-desktop-8K">dataset</a>]								
								[<a href="https://huggingface.co/spaces/showlab/ShowUI">demo</a>]								
								<br>
								<a style="color:#FA8072">Outstanding Paper Award, NeurIPS Open-World Agents Workshop 2024.</a><br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/videogui.jpg" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://showlab.github.io/videogui/"><b>VideoGUI: A Benchmark for GUI Automation from Instructional Videos</b></a><br>
								<u>Kevin QH. Lin</u>, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>NeurIPS D&B, </em>2024. <a style="color:#FA8072">Spotlight</a><br>
								[<a href="https://showlab.github.io/videogui/">project</a>]
								[<a href="https://showlab.github.io/videogui/assets/preprint.pdf">paper</a>]
								[<a href="https://github.com/showlab/videogui">code</a>]								
								<br>
								<!-- <a style="color:#FA8072">Can an agent recreate PowerPoint animation effects from instructional videos?</a><br> -->

						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/movieseq.png" width="240px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="https://arxiv.org/abs/2407.21757"><b>Learning Video Context as Interleaved Multimodal Sequences </b></a> <br><u>Kevin QH. Lin</u>, Pengchuan Zhang, Difei Gao, Xide Xia, Joya Chen, Ziteng Gao, Jinheng Xie, Xuhong Xiao, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>ECCV, </em>2024<br>
								[<a href="https://arxiv.org/abs/2407.21757">paper</a>]
								[<a href="https://github.com/showlab/MovieSeq">code</a>]
								<br>
								<!-- <a style="color:#FA8072">Video in-context learning using interleaved sequences of images, videos, plots and dialogues.</a><br> -->
								<br>
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="figures/univtg.jpg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://arxiv.org/abs/2307.16715"><b>UniVTG: Towards Unified Video-Language Temporal Grounding</b></a> <br>
							<u>Kevin QH. Lin</u>, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex JP. Wang, Rui Yan, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>International Conference on Computer Vision (<b>ICCV</b>), </em>2023.<br> -->
								<em>ICCV, </em>2023<br>
								[<a href="https://huggingface.co/spaces/KevinQHLin/UniVTG">demo</a>]
								[<a href="https://arxiv.org/pdf/2307.16715.pdf">paper</a>]
								[<a href="https://github.com/showlab/UniVTG">code</a>]
								<br>
							<!-- <font color="#FF0000">The first video temporal grounding pretraining model, unifying diverse temporal annotations to power moment retrieval, highlight detection and video summarization.</font> -->
							<!-- <a style="color:#FA8072">The first video temporal grounding pretraining model, unifying diverse temporal labels.</a><br> -->
						</td>
					</tr>
					<tr>
						<td width="260">
							<img src="EgoVLP/static/images/framework.jpeg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<!-- <td><a href="https://arxiv.org/abs/2206.01670"><b>Egocentric Video-Language Pretraining</b></a> -->
						<td><a href="https://arxiv.org/abs/2206.01670"><b>Egocentric Video-Language Pretraining</b></a>
							<br>
							<u>Kevin QH. Lin</u>, Alex JP. Wang, M. Soldan, M. Wray, R. Yan, Eric ZC. Xu, D. Gao, R. Tu,
							W. Zhao, W. Kong, C. Cai, H. Wang, D. Damen, B. Ghanem, W. Liu, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Neural Information Processing Systems (<b>NeurIPS</b>), </em>2022. <font
									color="#FF0000"><b>Spotlight (1.7%)</b></font><br> -->
								<em>NeurIPS, </em>2022. <a style="color:#FA8072">Spotlight (1.7%)</a><br>
								[<a href="https://qinghonglin.github.io/EgoVLP/">project</a>]
								[<a href="https://arxiv.org/pdf/2206.01670.pdf">paper</a>]
								[<a href="https://github.com/showlab/EgoVLP">code</a>]
								[<a href="EgoVLP/static/images/poster.pdf">poster</a>]
								<br>
								<!-- <font color="#FF0000">The first egocentric vision-language pretrained model.<br> -->
								<!-- <a style="color:#FA8072">The first egocentric vision-language pretrained model. </a><br> -->
								<a style="color:#FA8072">EgoVis Distinguished Paper Award & PREMIA Best Student Paper Award 2023.</a><br>
								<a style="color:#FA8072">Double champions in Ego4D & Epic-Kitchens CVPR 2022 challenges.</a><br>
									<!-- Double champions in Ego4D & Epic-Kitchens CVPR 2022 challenges.</font> <a href=https://cde.nus.edu.sg/ece/news-detail/double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges/>[News]</a> -->
						</td>
					</tr>
					<tr></tr>
					<tr></tr>
				</tbody>
			</table>
			</ul>

			<!-- <h2>Projects for fun</h2>
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="260">
							<img src="figures/vlog.jpeg" width="240px" height="120px"
								style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a
								href="https://github.com/showlab/VLog"><b>VLog: Video as a Long Document</b></a> <br>
							<p style="margin-top:3px">
								[<a href="https://huggingface.co/spaces/TencentARC/VLog">demo</a>]
								[<a href="https://github.com/showlab/VLog">code</a>]
								[<a href="https://twitter.com/KevinQHLin/status/1649124447037841408">twitter</a>]
								<br>
								Given a long video, we turn it into a doc containing visual + audio info. By sending this doc to ChatGPT, we can chat over the video!
						</td>
					</tr>
					<tr></tr>
					<tr></tr>
				</tbody>
			</table>
			</ul> -->

			<h2>Honors</h2>
			<ul>
				<li>
					<div style="float:left; text-align:left">Outstanding Paper Award, NeurIPS Open-World Agents</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">NeurIPS Top Reviewers</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Best Demo Paper Award, ACM Multimedia HCMA</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Egocentric Vision (EgoVis) Distinguished Paper Award</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">CVPR Outstanding Reviewers (Top 2%)</div>
					<div style="float:right; text-align:right">2024</div>
				</li>				
				<li>
					<div style="float:left; text-align:left">PREMIA Best Student Paper Awards, Gold Award</div>
					<div style="float:right; text-align:right">2023</div>
				</li>
				<li>
					<div style="float:left; text-align:left">NeurIPS Scholar Award</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<!-- <div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship <a href=https://mp.weixin.qq.com/s/06e7m8twMen9DzrAE0raPA>[News]</a></div> -->
					<div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship, Second Prize</a></div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">1st Place on Ego4D - Object State Change Classiﬁcation
						Challenge, CVPR</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left"></div>1st Place on EPIC-Kitchens - Multi-Instance Retrieval
					Challenge, CVPR<div style="float:right; text-align:right">2022</div>
				</li>
				<!--
	<li>
		<div style="float:left; text-align:left">2nd Place on Ego4D - Natural Language Queries Challenge, CVPR</div> <div style="float:right; text-align:right">2022</div>
	</li>
-->
				<!-- <li>
					<div style="float:left; text-align:left">AAAI Student Scholarship</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">SIGIR Student Travel Grant</div>
					<div style="float:right; text-align:right">2021</div>
				</li> -->
				<!--	<li>
		<div style="float:left; text-align:left">Outstanding Graduate at SZU</div> <div style="float:right; text-align:right">2022, 2019</div>
	</li>
-->
				<li>
					<div style="float:left; text-align:left">Show Lab Annual Award</div>
					<div style="float:right; text-align:right">2022, 2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">China National Scholarship</div>
					<div style="float:right; text-align:right">2018, 2021</div>
				</li>
			</ul>


			<h2>Service</h2>
			<ul>
				<li>
					<!-- SDM -->
					<p>Conference Reviewer: CVPR (2024 Outstanding Reviewers), ICCV, ECCV, NeurIPS (2024 Top Reviewers), ICML, ICLR, etc.</p> 
				</li>
				<li>
					<p>Journal Reviewer: TPAMI, IJCV, TNNLS, TMM, etc.</p>
				</li>
				<li>
					<p>Teaching Assistant: 
						<a href="https://nusmods.com/courses/EE6934/deep-learning-advanced">EE6934 Deep Learning</a>, 
						<a href="https://nusmods.com/courses/EE6733/advanced-topics-on-vision-and-machine-learning">EE6733 Advanced Topics on Vision and Machine Learning</a>
					</p>
				</li>
				<li>
					<p>Co-organizer of <a href="https://theaitalks.org/">The AI Talks.</a>
				</li>
			</ul>

			</br>
			<div align="center">
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/count2/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a></div> -->
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/map/yZuS/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a></div> -->
				<a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/mini/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"></a>
		</div>
		</br>

		<div align="center">

			<body>
				<font color="gray">&copy Kevin</font>
			</body>
		</div>

		<script>
			(function (i, s, o, g, r, a, m) {
				i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
					(i[r].q = i[r].q || []).push(arguments)
				}, i[r].l = 1 * new Date(); a = s.createElement(o),
					m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
			})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

			ga('create', 'UA-88615920-1', 'auto');
			ga('send', 'pageview');
		</script>
	</div>
	</div>
</body>

</html>
